{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd \n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d672d7a",
   "metadata": {},
   "source": [
    "### 1. Loading Engagement Matrix\n",
    "\n",
    "This function loads the engagement matrix LL from a .mat file and handles missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a70a6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_engagement_matrix(file_path):\n",
    "    \"\"\"\n",
    "    Load the engagement matrix L from the MATLAB .mat file using the key 'train'.\n",
    "    \"\"\"\n",
    "    data = loadmat(file_path)\n",
    "    if 'train' in data:\n",
    "        L = data['train']\n",
    "    else:\n",
    "        raise KeyError(\"The matrix 'train' is not found in the .mat file.\")\n",
    "    L = np.array(L, dtype=float)\n",
    "    L[L == 0] = np.nan  # Replace 0s with NaN to handle missing values\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804a845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embeddings_xavier(num_researchers, num_papers, embedding_dim, seed=42):\n",
    "    \"\"\"\n",
    "    Initialize embeddings for researchers and papers using Xavier initialization.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    limit = np.sqrt(6 / (2 * embedding_dim))\n",
    "    researcher_vecs = np.random.uniform(-limit, limit, (num_researchers, embedding_dim))\n",
    "    papers_vecs = np.random.uniform(-limit, limit, (num_papers, embedding_dim))\n",
    "    return researcher_vecs, papers_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d680ce",
   "metadata": {},
   "source": [
    "### 2. Initializing Embeddings\n",
    "\n",
    "Randomly initializes embeddings for researchers and papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee170b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embeddings(num_researchers, num_papers, embedding_dim, seed=42):\n",
    "    \"\"\"\n",
    "    Initialize embeddings for researchers and papers.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    researcher_vecs = np.random.normal(scale=0.1, size=(num_researchers, embedding_dim))\n",
    "    papers_vecs = np.random.normal(scale=0.1, size=(num_papers, embedding_dim))\n",
    "    return researcher_vecs, papers_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08eb4b",
   "metadata": {},
   "source": [
    "### 3. Updating Embeddings\n",
    "##### 3.1 Researcher Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0109b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg):\n",
    "    \"\"\"\n",
    "    Update researcher embeddings (Z) given paper embeddings (W).\n",
    "    \"\"\"\n",
    "    num_researchers, embedding_dim = researcher_vecs.shape\n",
    "    for i in range(num_researchers):\n",
    "        indices = ~np.isnan(L[i, :])\n",
    "        if np.sum(indices) > 0:\n",
    "            W_obs = papers_vecs[indices]\n",
    "            L_obs = L[i, indices]\n",
    "            A = W_obs.T @ W_obs + lambda_reg * np.eye(embedding_dim)\n",
    "            b = W_obs.T @ L_obs\n",
    "            researcher_vecs[i] = np.linalg.solve(A, b)\n",
    "    return researcher_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea53ec",
   "metadata": {},
   "source": [
    "####  3.2 Paper Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cc9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg):\n",
    "    \"\"\"\n",
    "    Update paper embeddings (W) given researcher embeddings (Z).\n",
    "    \"\"\"\n",
    "    num_papers, embedding_dim = papers_vecs.shape\n",
    "    for j in range(num_papers):\n",
    "        indices = ~np.isnan(L[:, j])\n",
    "        if np.sum(indices) > 0:\n",
    "            Z_obs = researcher_vecs[indices]\n",
    "            L_obs = L[indices, j]\n",
    "            A = Z_obs.T @ Z_obs + lambda_reg * np.eye(embedding_dim)\n",
    "            b = Z_obs.T @ L_obs\n",
    "            papers_vecs[j] = np.linalg.solve(A, b)\n",
    "    return papers_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8f6f0",
   "metadata": {},
   "source": [
    "### 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "336c0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_matrix_factorization(L, embedding_dim, lambda_reg, num_iterations):\n",
    "    \"\"\"\n",
    "    Train matrix factorization using alternating minimization.\n",
    "    \"\"\"\n",
    "    num_researchers, num_papers = L.shape\n",
    "    researcher_vecs, papers_vecs = initialize_embeddings(num_researchers, num_papers, embedding_dim)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        researcher_vecs = update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "        papers_vecs = update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "        mse_loss = get_train_mse(L, researcher_vecs, papers_vecs)\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, MSE Loss: {mse_loss:.4f}\")\n",
    "\n",
    "    return researcher_vecs, papers_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c8f7ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_matrix_factorization(L, embedding_dim, lambda_reg, num_iterations):\n",
    "    \"\"\"\n",
    "    Train matrix factorization using alternating minimization with Xavier initialization.\n",
    "    \"\"\"\n",
    "    num_researchers, num_papers = L.shape\n",
    "    researcher_vecs, papers_vecs = initialize_embeddings_xavier(num_researchers, num_papers, embedding_dim)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        researcher_vecs = update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "        papers_vecs = update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "        mse_loss = get_train_mse(L, researcher_vecs, papers_vecs)\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, MSE Loss: {mse_loss:.4f}\")\n",
    "    \n",
    "    return researcher_vecs, papers_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38514b",
   "metadata": {},
   "source": [
    "### 5. Evaluation Metrics\n",
    "##### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cb00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_mse(L, researcher_vecs, papers_vecs):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) loss on the training data.\n",
    "    \"\"\"\n",
    "    mse_loss = 0    \n",
    "    for i in range(L.shape[0]):\n",
    "        for j in range(L.shape[1]):\n",
    "            if not np.isnan(L[i, j]):\n",
    "                mse_loss += (np.dot(researcher_vecs[i], papers_vecs[j]) - L[i, j])**2\n",
    "    return mse_loss / np.sum(~np.isnan(L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767d9c0",
   "metadata": {},
   "source": [
    "###  Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f863d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_acc(L, researcher_vecs, papers_vecs):\n",
    "    \"\"\"\n",
    "    Calculate training accuracy using a positive/negative metric.\n",
    "    \"\"\"\n",
    "    num_correct, total = 0, 0\n",
    "    for i in range(L.shape[0]):\n",
    "        for j in range(L.shape[1]):\n",
    "            if not np.isnan(L[i, j]):\n",
    "                total += 1\n",
    "                approx = np.dot(researcher_vecs[i], papers_vecs[j])\n",
    "                if approx * L[i, j] > 0:\n",
    "                    num_correct += 1             \n",
    "    return num_correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584fa84",
   "metadata": {},
   "source": [
    "test test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365bd07",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "729a75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded engagement matrix L with shape (24983, 100)\n",
      "Iteration 1/20, MSE Loss: 18.4077\n",
      "Iteration 2/20, MSE Loss: 12.7469\n",
      "Iteration 3/20, MSE Loss: 11.6793\n",
      "Iteration 4/20, MSE Loss: 11.1750\n",
      "Iteration 5/20, MSE Loss: 10.9049\n",
      "Iteration 6/20, MSE Loss: 10.7517\n",
      "Iteration 7/20, MSE Loss: 10.6542\n",
      "Iteration 8/20, MSE Loss: 10.5847\n",
      "Iteration 9/20, MSE Loss: 10.5307\n",
      "Iteration 10/20, MSE Loss: 10.4866\n",
      "Iteration 11/20, MSE Loss: 10.4495\n",
      "Iteration 12/20, MSE Loss: 10.4180\n",
      "Iteration 13/20, MSE Loss: 10.3913\n",
      "Iteration 14/20, MSE Loss: 10.3689\n",
      "Iteration 15/20, MSE Loss: 10.3505\n",
      "Iteration 16/20, MSE Loss: 10.3354\n",
      "Iteration 17/20, MSE Loss: 10.3232\n",
      "Iteration 18/20, MSE Loss: 10.3132\n",
      "Iteration 19/20, MSE Loss: 10.3051\n",
      "Iteration 20/20, MSE Loss: 10.2983\n",
      "Final Training MSE: 10.2983\n",
      "Final Training Accuracy: 0.8189\n"
     ]
    }
   ],
   "source": [
    "# Load the engagement matrix L\n",
    "file_path = 'papers_train.mat'  # Replace with the actual file path\n",
    "L = load_engagement_matrix(file_path)\n",
    "print(f\"Loaded engagement matrix L with shape {L.shape}\")\n",
    "\n",
    "# Train Matrix Factorization\n",
    "embedding_dim = 10  # Dimensionality of embeddings\n",
    "lambda_reg = 0.1    # Regularization strength\n",
    "num_iterations = 20 # Number of training iterations\n",
    "\n",
    "researcher_vecs, papers_vecs = train_matrix_factorization(L, embedding_dim, lambda_reg, num_iterations)\n",
    "\n",
    "# Evaluate the Model\n",
    "mse_loss = get_train_mse(L, researcher_vecs, papers_vecs)\n",
    "train_acc = get_train_acc(L, researcher_vecs, papers_vecs)\n",
    "print(f\"Final Training MSE: {mse_loss:.4f}\")\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f889b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in the engagement matrix L\n",
    "def impute_matrix(L):\n",
    "    \"\"\"\n",
    "    Impute missing values (NaN) in the matrix L using the row mean.\n",
    "    \"\"\"\n",
    "    num_researchers = L.shape[0]\n",
    "    for i in range(num_researchers):\n",
    "        row = L[i, :]\n",
    "        row_mean = np.nanmean(row)\n",
    "        row[np.isnan(row)] = row_mean\n",
    "    return L\n",
    "L_imputed = impute_matrix(L) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906495f",
   "metadata": {},
   "source": [
    "### 1. Updating Embeddings\n",
    "##### 1.1 Researcher Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4079f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg):\n",
    "    \"\"\"\n",
    "    Update researcher embeddings (Z) given paper embeddings (W).\n",
    "    \"\"\"\n",
    "    num_researchers, embedding_dim = researcher_vecs.shape\n",
    "    for i in range(num_researchers):\n",
    "        indices = ~np.isnan(L[i, :])  # Observed indices for researcher i\n",
    "        if np.sum(indices) > 0:\n",
    "            W_obs = papers_vecs[indices]  # Paper embeddings for observed entries\n",
    "            L_obs = L[i, indices]  # Observed engagement values\n",
    "            A = W_obs.T @ W_obs + lambda_reg * np.eye(embedding_dim)\n",
    "            b = W_obs.T @ L_obs\n",
    "            researcher_vecs[i] = np.linalg.solve(A, b)\n",
    "    return researcher_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a27f10",
   "metadata": {},
   "source": [
    "##### 1.2 Paper Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7379e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg):\n",
    "    \"\"\"\n",
    "    Update paper embeddings (W) given researcher embeddings (Z).\n",
    "    \"\"\"\n",
    "    num_papers, embedding_dim = papers_vecs.shape\n",
    "    for j in range(num_papers):\n",
    "        indices = ~np.isnan(L[:, j])  # Observed indices for paper j\n",
    "        if np.sum(indices) > 0:\n",
    "            Z_obs = researcher_vecs[indices]  # Researcher embeddings for observed entries\n",
    "            L_obs = L[indices, j]  # Observed engagement values\n",
    "            A = Z_obs.T @ Z_obs + lambda_reg * np.eye(embedding_dim)\n",
    "            b = Z_obs.T @ L_obs\n",
    "            papers_vecs[j] = np.linalg.solve(A, b)\n",
    "    return papers_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904625fe",
   "metadata": {},
   "source": [
    "###  2. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a375bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_matrix_factorization2(L, embedding_dim, lambda_reg, num_iterations):\n",
    "    \"\"\"\n",
    "    Train matrix factorization using alternating minimization.\n",
    "    \"\"\"\n",
    "    num_researchers, num_papers = L.shape\n",
    "    researcher_vecs, papers_vecs = initialize_embeddings_xavier(num_researchers, num_papers, embedding_dim)\n",
    "    \n",
    "    start_time = time()\n",
    "    for iteration in range(num_iterations):\n",
    "        researcher_vecs = update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "        papers_vecs = update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "        mse_loss = get_train_mse(L, researcher_vecs, papers_vecs)\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, MSE Loss: {mse_loss:.4f}\")\n",
    "    \n",
    "    total_time = time() - start_time\n",
    "    print(f\"Training completed in {total_time:.2f} seconds\")\n",
    "    return researcher_vecs, papers_vecs\n",
    "\n",
    "#def train_matrix_factorization(L, embedding_dim, lambda_reg, num_iterations):\n",
    "#    \"\"\"\n",
    "#    Train matrix factorization using alternating minimization with Xavier initialization.\n",
    "#    \"\"\"\n",
    "#    num_researchers, num_papers = L.shape\n",
    "#    # Use Xavier initialization\n",
    "#    researcher_vecs, papers_vecs = initialize_embeddings(num_researchers, num_papers, embedding_dim)\n",
    "#    \n",
    "#    for iteration in range(num_iterations):\n",
    "#        researcher_vecs = update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "#        papers_vecs = update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "#        mse_loss = get_train_acc(L, researcher_vecs, papers_vecs)\n",
    "#        print(f\"Iteration {iteration + 1}/{num_iterations}, MSE Loss: {mse_loss:.4f}\")\n",
    "#    \n",
    "#    return researcher_vecs, papers_vecs\n",
    "\n",
    "#def get_weighted_train_mse(L, researcher_vecs, papers_vecs):\n",
    "#    \"\"\"\n",
    "#    Calculate the Weighted Mean Squared Error (MSE) loss on the training data.\n",
    "#    \"\"\"\n",
    "#    mse_loss, total_weight = 0, 0\n",
    "#    for i in range(L.shape[0]):\n",
    "#        for j in range(L.shape[1]):\n",
    "#            if not np.isnan(L[i, j]):\n",
    "#                weight = 1 + abs(L[i, j])  # Higher weight for larger engagement values\n",
    "#                total_weight += weight\n",
    "#                mse_loss += weight * (np.dot(researcher_vecs[i], papers_vecs[j]) - L[i, j])**2\n",
    "#    return mse_loss / total_weight\n",
    "#\n",
    "\n",
    "#def train_matrix_factorization_with_weighted_mse(L, embedding_dim, lambda_reg, num_iterations):\n",
    "#    \"\"\"\n",
    "#    Train matrix factorization using alternating minimization with Weighted MSE.\n",
    "#    \"\"\"\n",
    "#    num_researchers, num_papers = L.shape\n",
    "#    researcher_vecs, papers_vecs = initialize_embeddings_xavier(num_researchers, num_papers, embedding_dim)\n",
    "#\n",
    "#    for iteration in range(num_iterations):\n",
    "#        # Update researcher and paper embeddings\n",
    "#        researcher_vecs = update_researcher_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "#        papers_vecs = update_paper_embeddings(L, researcher_vecs, papers_vecs, lambda_reg)\n",
    "#        \n",
    "#        # Calculate weighted MSE\n",
    "#        weighted_mse_loss = get_weighted_train_mse(L, researcher_vecs, papers_vecs)\n",
    "#        print(f\"Iteration {iteration + 1}/{num_iterations}, Weighted MSE Loss: {weighted_mse_loss:.4f}\")\n",
    "#    \n",
    "#    return researcher_vecs, papers_vecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81ae823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_matrix(L):\n",
    "    \"\"\"\n",
    "    Impute missing values (NaN) in the matrix L using the row mean.\n",
    "    \"\"\"\n",
    "    num_researchers = L.shape[0]\n",
    "    for i in range(num_researchers):\n",
    "        row = L[i, :]\n",
    "        row_mean = np.nanmean(row)\n",
    "        row[np.isnan(row)] = row_mean\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9dc259",
   "metadata": {},
   "source": [
    "### 3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8adba20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/20, MSE Loss: 7.2724\n",
      "Iteration 2/20, MSE Loss: 5.9310\n",
      "Iteration 3/20, MSE Loss: 5.7745\n",
      "Iteration 4/20, MSE Loss: 5.6953\n",
      "Iteration 5/20, MSE Loss: 5.6588\n",
      "Iteration 6/20, MSE Loss: 5.6415\n",
      "Iteration 7/20, MSE Loss: 5.6323\n",
      "Iteration 8/20, MSE Loss: 5.6268\n",
      "Iteration 9/20, MSE Loss: 5.6233\n",
      "Iteration 10/20, MSE Loss: 5.6207\n",
      "Iteration 11/20, MSE Loss: 5.6186\n",
      "Iteration 12/20, MSE Loss: 5.6169\n",
      "Iteration 13/20, MSE Loss: 5.6154\n",
      "Iteration 14/20, MSE Loss: 5.6141\n",
      "Iteration 15/20, MSE Loss: 5.6129\n",
      "Iteration 16/20, MSE Loss: 5.6118\n",
      "Iteration 17/20, MSE Loss: 5.6108\n",
      "Iteration 18/20, MSE Loss: 5.6099\n",
      "Iteration 19/20, MSE Loss: 5.6091\n",
      "Iteration 20/20, MSE Loss: 5.6084\n",
      "Training completed in 224.47 seconds\n",
      "Final Training Accuracy: 0.8634\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "embedding_dim = 10\n",
    "lambda_reg = 0.1\n",
    "num_iterations = 20\n",
    "L = load_engagement_matrix('papers_train.mat')\n",
    "L1 = impute_matrix(L)\n",
    "researcher_vecs, papers_vecs = train_matrix_factorization2(L1, embedding_dim, lambda_reg, num_iterations)\n",
    "train_acc = get_train_acc(L1, researcher_vecs, papers_vecs)\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa19a2c",
   "metadata": {},
   "source": [
    "### Generating Predictions for Researcher-Paper Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfc524",
   "metadata": {},
   "source": [
    "####  1. Loading the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6cd79776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_set_from_txt(file_path):\n",
    "    \"\"\"\n",
    "    Load the test set from a text file with researcher-paper pairs.\n",
    "    \"\"\"\n",
    "    test_data = pd.read_csv(file_path, header=None, names=[\"Researcher\", \"Paper\"])\n",
    "    test_data[\"ID\"] = range(1, len(test_data) + 1)  # Add unique IDs for each row\n",
    "    return test_data[[\"ID\", \"Researcher\", \"Paper\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830a684",
   "metadata": {},
   "source": [
    "#### 2. Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e7ce919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(test_data, researcher_vecs, papers_vecs):\n",
    "    \"\"\"\n",
    "    Generate predictions for the test set.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        researcher_id = int(row[\"Researcher\"]) - 1  # Adjusting for 0-based indexing\n",
    "        paper_id = int(row[\"Paper\"]) - 1           # Adjusting for 0-based indexing\n",
    "        dot_product = np.dot(researcher_vecs[researcher_id], papers_vecs[paper_id])\n",
    "        predicted_label = 1 if dot_product > 0 else -1\n",
    "        predictions.append(predicted_label)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419694fd",
   "metadata": {},
   "source": [
    "#### Saving Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d08a0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_csv(test_data, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Save predictions to a CSV file in the Kaggle-required format.\n",
    "    \"\"\"\n",
    "    test_data[\"Value\"] = predictions\n",
    "    submission = test_data[[\"ID\", \"Value\"]]\n",
    "    submission.to_csv(output_file, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4592333",
   "metadata": {},
   "source": [
    "#### Steps to Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9228863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to done.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the test set\n",
    "test_file = 'papers_test.txt'  # Replace with the actual test file path\n",
    "test_data = load_test_set_from_txt(test_file)\n",
    "\n",
    "\n",
    "# Step 2: Generate predictions\n",
    "predictions = generate_predictions(test_data, researcher_vecs, papers_vecs)\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "output_file = 'done.csv'  # Desired output file name\n",
    "save_predictions_to_csv(test_data, predictions, output_file)\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
